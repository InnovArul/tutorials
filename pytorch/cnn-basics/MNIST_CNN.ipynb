{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten digits classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![l](./pics/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- For obtaining the plots in the notebook -----#\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch module spefic to com puter vision\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- Basic setup ------- #\n",
    "DATASET_DIRECTORY = './mnist/'\n",
    "SAVE_DIRECTORY    = './models/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#------- Download training data, and set transformations to be applied -----#\n",
    "mnist_train = datasets.MNIST(DATASET_DIRECTORY,\n",
    "                             train=True,\n",
    "                             transform=transforms.ToTensor(),\n",
    "                             target_transform=None,\n",
    "                             download=True)\n",
    "\n",
    "mnist_test  = datasets.MNIST(DATASET_DIRECTORY,\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor(),\n",
    "                             target_transform = None,\n",
    "                             download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions\n",
      "torch.Size([60000, 28, 28])\n",
      "Test dataset dimensions\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# ---- Dimensions of dataset ---- # \n",
    "# ---- Training dataset ---- #\n",
    "print(\"Training dataset dimensions\")\n",
    "print(mnist_train.train_data.size())\n",
    "\n",
    "# ---- Test dataset ---- #\n",
    "print(\"Test dataset dimensions\")\n",
    "print(mnist_test.test_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f99aac979e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADgRJREFUeJzt3X+MVPW5x/HPI4ImFBOVdEWR0jZ6TWMivdnoTUSlaSWo\nNdA/YOtPmjbdotVcjDGX6B+aNDWmKdYmJoQlYrcNpTTxF2luhJZot8basBDq71YuobIbfkgodhtF\n7sLTP+Zsu9Wd78zOnJlzhuf9SjY7c545Mw8TPnvOme+c8zV3F4B4Tiu6AQDFIPxAUIQfCIrwA0ER\nfiAowg8ERfiBoAg/EBThB4I6vZ0vZmZ8nRBoMXe3eh7X1JbfzBaZ2Z/MbLeZrWrmuQC0lzX63X4z\nmyLpz5KulTQkabukm9z9zcQ6bPmBFmvHlv9ySbvdfY+7H5f0C0mLm3g+AG3UTPgvkLRv3P2hbNm/\nMbNeMxs0s8EmXgtAzlr+gZ+790nqk9jtB8qkmS3/sKQLx92fnS0D0AGaCf92SReZ2WfNbJqkr0va\nnE9bAFqt4d1+dx81s7skbZE0RdJ6d38jt84AtFTDQ30NvRjH/EDLteVLPgA6F+EHgiL8QFCEHwiK\n8ANBEX4gqLaez4/2e/DBB5P122+/PVnv6elJ1gcHOWWjU7HlB4Ii/EBQhB8IivADQRF+ICjCDwTF\nUN8pYMGCBVVrvb29yXU/+OCDZL27uztZZ6ivc7HlB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGguHpv\nB5gxY0ayvmfPnqq1/v7+5LqrVqUnV671/+PEiRPJOtqPq/cCSCL8QFCEHwiK8ANBEX4gKMIPBEX4\ngaCaOp/fzPZKGpF0QtKou6dP/kZD7rjjjmT92LFjVWurV69Orjs6OtpQT+h8eVzM40vufjiH5wHQ\nRuz2A0E1G36XtNXMdphZ+npRAEql2d3++e4+bGaflvRrM3vb3QfGPyD7o8AfBqBkmtryu/tw9vuQ\npGckXT7BY/rcvZsPA4FyaTj8ZjbdzGaM3Za0UNLreTUGoLWa2e3vkvSMmY09z8/d/flcugLQcpzP\n3wEOH06PpK5du7Zq7YEHHsi7HZQc5/MDSCL8QFCEHwiK8ANBEX4gKMIPBMUU3SVQ69LcZ5xxRrL+\n9ttv59kOgmDLDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc5fAosWLWpq/eef5zIKmDy2/EBQhB8I\nivADQRF+ICjCDwRF+IGgCD8QFOP8JbBixYpk/aOPPkrW33vvvTzbQRBs+YGgCD8QFOEHgiL8QFCE\nHwiK8ANBEX4gqJrj/Ga2XtJXJR1y90uzZedI2iRprqS9kpa5+19b12ZnM0vPmHzuuecm69u2bcuz\nndJYsGBBst7T09PU8x89erRqbWBgILlurWsktHNq+1apZ8v/E0kfv9rEKknb3P0iSduy+wA6SM3w\nu/uApCMfW7xYUn92u1/Skpz7AtBijR7zd7n7/uz2AUldOfUDoE2a/m6/u7uZVT0AMrNeSb3Nvg6A\nfDW65T9oZrMkKft9qNoD3b3P3bvdvbvB1wLQAo2Gf7Ok5dnt5ZKey6cdAO1SM/xmtlHS7yX9h5kN\nmdm3JD0i6Voze0fSV7L7ADqItXO8MvXZwKns/PPPT9aHhoaS9VtuuSVZ37hx46R7ysu0adOS9Uce\nqb5dWLlyZXLdd999N1kfGRlpeP358+cn1126dGmyvnXr1mS9SO6e/mJJhm/4AUERfiAowg8ERfiB\noAg/EBThB4Li0t0doMhLc592Wnr7sG7dumT9tttuq1q78847k+s++eSTyXqtS5qnLFmSPhdt7dq1\nyfq8efOS9ffff3/SPbUbW34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIpx/jaYM2dOU+tv3749p04m\n7/HHH0/WFy5c2HC91iXJW3m6+ZYtW5L1M888M1mfPn16ss44P4DSIvxAUIQfCIrwA0ERfiAowg8E\nRfiBoBjnb4OurvJOZXjeeecl6zfeeGOyfvPNNyfrL7zwwqR7aocPP/wwWd+9e3eyftVVVyXrmzZt\nmnRP7caWHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjnOb2brJX1V0iF3vzRb9pCkb0sau6D8/e7+\nv61qstMdP368qfVnz56drDdz7vitt96arNf6HsDLL7/c8Gt3shkzZhTdQtPq2fL/RNKiCZb/yN3n\nZT8EH+gwNcPv7gOSjrShFwBt1Mwx/11m9qqZrTezs3PrCEBbNBr+NZI+L2mepP2SVld7oJn1mtmg\nmQ02+FoAWqCh8Lv7QXc/4e4nJa2TdHnisX3u3u3u3Y02CSB/DYXfzGaNu/s1Sa/n0w6AdqlnqG+j\npAWSZprZkKQHJS0ws3mSXNJeSd9pYY8AWqBm+N39pgkWP9GCXk5ZL730UrJ+4MCBZH3FihXJ+t13\n3z3pnsa88soryfrpp6f/i1xzzTXJ+tatWyfdUzvU+nedddZZyfrRo0fzbKcQfMMPCIrwA0ERfiAo\nwg8ERfiBoAg/EBSX7m6DkZGRZH14eDhZX7p0abJ+zz33VK2Njo4m1z1yJH3O1smTJ5P1KVOmJOtl\nVWt4tNapzLWmF+8EbPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IChz9/a9mFn7XqyD9PT0JOsbNmxI\n1tesWVO11szpvpLU19eXrN9www3J+vr166vWjh071lBPY2qdKj1nzpyqtXXr1iXXve6665L1sk49\nLknubvU8ji0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOH8H2LRpU7K+ZMmSqrXHHnssue6jjz6a\nrNea/nvRookmcP6XmTNnVq2ZpYejp02blqxffPHFyfpll11WtXbvvfcm192xY0eyXmaM8wNIIvxA\nUIQfCIrwA0ERfiAowg8ERfiBoGqO85vZhZJ+KqlLkkvqc/cfm9k5kjZJmitpr6Rl7v7XGs/FOH8D\npk6dmqw//PDDVWsrV65MrltrzoBnn302Wd+3b1+ynpL6foIkXXnllcl6rWvn33fffVVru3btSq7b\nyfIc5x+VdK+7f0HSf0n6rpl9QdIqSdvc/SJJ27L7ADpEzfC7+35335ndHpH0lqQLJC2W1J89rF9S\n+s84gFKZ1DG/mc2V9EVJf5DU5e77s9IBVQ4LAHSIuufqM7NPSXpK0kp3/9v472W7u1c7njezXkm9\nzTYKIF91bfnNbKoqwd/g7k9niw+a2aysPkvSoYnWdfc+d+929+48GgaQj5rht8om/glJb7n7+FPA\nNktant1eLum5/NsD0Cr1DPXNl/Q7Sa9JGpuv+X5Vjvt/KWmOpL+oMtSXnO+Zob72u+KKK5L1ZcuW\nJetXX311sn7JJZck6y+++GLV2s6dO5PrDgwMJOu1Lp9da3rxU1W9Q301j/nd/SVJ1Z7sy5NpCkB5\n8A0/ICjCDwRF+IGgCD8QFOEHgiL8QFBcuhs4xXDpbgBJhB8IivADQRF+ICjCDwRF+IGgCD8QFOEH\ngiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTN\n8JvZhWb2gpm9aWZvmNl/Z8sfMrNhM9uV/Vzf+nYB5KXmpB1mNkvSLHffaWYzJO2QtETSMkl/d/cf\n1v1iTNoBtFy9k3acXscT7Ze0P7s9YmZvSbqgufYAFG1Sx/xmNlfSFyX9IVt0l5m9ambrzezsKuv0\nmtmgmQ021SmAXNU9V5+ZfUrSbyV9392fNrMuSYcluaTvqXJo8M0az8FuP9Bi9e721xV+M5sq6VeS\ntrj7oxPU50r6lbtfWuN5CD/QYrlN1GlmJukJSW+ND372QeCYr0l6fbJNAihOPZ/2z5f0O0mvSTqZ\nLb5f0k2S5qmy279X0neyDwdTz8WWH2ixXHf780L4gdbLbbcfwKmJ8ANBEX4gKMIPBEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTNC3jm7LCkv4y7PzNbVkZl7a2sfUn01qg8e/tM\nvQ9s6/n8n3hxs0F37y6sgYSy9lbWviR6a1RRvbHbDwRF+IGgig5/X8Gvn1LW3sral0RvjSqkt0KP\n+QEUp+gtP4CCFBJ+M1tkZn8ys91mtqqIHqoxs71m9lo283ChU4xl06AdMrPXxy07x8x+bWbvZL8n\nnCatoN5KMXNzYmbpQt+7ss143fbdfjObIunPkq6VNCRpu6Sb3P3NtjZShZntldTt7oWPCZvZ1ZL+\nLumnY7MhmdkPJB1x90eyP5xnu/v/lKS3hzTJmZtb1Fu1maW/oQLfuzxnvM5DEVv+yyXtdvc97n5c\n0i8kLS6gj9Jz9wFJRz62eLGk/ux2vyr/edquSm+l4O773X1ndntE0tjM0oW+d4m+ClFE+C+QtG/c\n/SGVa8pvl7TVzHaYWW/RzUyga9zMSAckdRXZzARqztzcTh+bWbo0710jM17njQ/8Pmm+u/+npOsk\nfTfbvS0lrxyzlWm4Zo2kz6syjdt+SauLbCabWfopSSvd/W/ja0W+dxP0Vcj7VkT4hyVdOO7+7GxZ\nKbj7cPb7kKRnVDlMKZODY5OkZr8PFdzPP7n7QXc/4e4nJa1Tge9dNrP0U5I2uPvT2eLC37uJ+irq\nfSsi/NslXWRmnzWzaZK+LmlzAX18gplNzz6IkZlNl7RQ5Zt9eLOk5dnt5ZKeK7CXf1OWmZurzSyt\ngt+70s147e5t/5F0vSqf+P+fpAeK6KFKX5+T9Mfs542ie5O0UZXdwP9X5bORb0k6V9I2Se9I+o2k\nc0rU289Umc35VVWCNqug3uarskv/qqRd2c/1Rb93ib4Ked/4hh8QFB/4AUERfiAowg8ERfiBoAg/\nEBThB4Ii/EBQhB8I6h8R7ZSOz/HkjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a62723588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Dislay some random image --- #\n",
    "imgindex = 100\n",
    "plt.imshow(mnist_test.test_data[imgindex].numpy(),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network\n",
    "\n",
    "![mnistnet](./pics/MNIST_CNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (256 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ---- Build the architecture --- #\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        #--- The blocks are defined here ---#\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5,stride=1,padding=0)\n",
    "        self.mp1   = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5,stride=1,padding=0)\n",
    "        self.mp2   = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.fc1   = nn.Linear(in_features=256,out_features=120)\n",
    "        self.fc2   = nn.Linear(in_features=120,out_features=84)\n",
    "        self.fc3  = nn.Linear(in_features=84,out_features=10)\n",
    "  \n",
    "        \n",
    "    #--- Forward propagation ---#\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mp1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mp2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(-1,256)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      " 7.2042e-04  5.2868e-06  1.9558e-05  3.1077e-01  2.3788e-10  1.5756e-01\n",
      "\n",
      "Columns 6 to 9 \n",
      " 7.9860e-08  2.8216e-03  5.2734e-01  7.6072e-04\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 28, 28))\n",
    "out = model(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "In this example, we will use Classification Cross-Entropy loss and SGD with momentum.<br>\n",
    "Cross Entropy loss is given as:- $L=-\\sum_i y_i \\log(p_i)$ and $p_i=\\frac{\\exp^{x_i}}{\\sum_k \\exp^{x_k}}$\n",
    "\n",
    "There are many other loss functions such as MSELoss, L1Loss etc. Visit [here](http://pytorch.org/docs/master/nn.html#loss-functions) for other loss functions.\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "$$w_{n+1} = w_{n} - \\eta \\triangle$$\n",
    "$$\\triangle = 0.9\\triangle + \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "Although SGD is the most popular and basic optimizer that one should first try. There are many adaptive optimizers like Adagrad,Adadelta RMSProp and many more. Visit [here](http://pytorch.org/docs/master/optim.html) for other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Settings for training ---- #\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data :  469  batches of 128 samples each\n",
      "Test data :  79  batches of 128 samples each\n"
     ]
    }
   ],
   "source": [
    "# ----- Dataloades getting datasets ready for the network ---- #\n",
    "BATCH_SIZE    = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "# ---- Knowing about how data is stored in the loader ---- #\n",
    "no_of_batches_train = len(train_loader)\n",
    "no_of_batches_test  = len(test_loader)\n",
    "print(\"Train data : \", no_of_batches_train, \" batches of 128 samples each\")\n",
    "print(\"Test data : \",no_of_batches_test, \" batches of 128 samples each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [08:47<00:00, 21.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Training procedure ---- #\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "# --- Iterate over epoch ---- #\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    \n",
    "    # ---- Iterate over each batch --- # \n",
    "    for iter_,data in enumerate(train_loader):\n",
    "        inputs,labels = data\n",
    "        \n",
    "        # --- If CUDA is available move the tensor to GPU --- #\n",
    "        if USE_CUDA:\n",
    "            inputs,labels = inputs.cuda(),labels.cuda()            \n",
    "        \n",
    "        # --- To perform back propagation the tensors should be placed in Variable --- #\n",
    "        inputs,labels = Variable(inputs),Variable(labels)\n",
    "        \n",
    "        # --- Setting the obtained gradients to zero --- #\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # --- Forward propagation --- #\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # --- Backward propagation and optimization --- #\n",
    "        # --- Finding the error --- # \n",
    "        loss = criterion(outputs,labels)\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        #--- Backward propogate ---#\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1 ---> Weights :  torch.Size([6, 1, 5, 5])\n",
      "FC1 ---> Bias    :  torch.Size([6])\n",
      "FC2 ---> Weights :  torch.Size([16, 6, 5, 5])\n",
      "FC2 ---> Bias    :  torch.Size([16])\n",
      "FC3 ---> Weights :  torch.Size([120, 256])\n",
      "FC3 ---> Bias    :  torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "#--- Parameter dimensions ---#\n",
    "#--- This includes bias and weights ---#\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "print(\"FC1 ---> Weights : \",params[0].size())\n",
    "print(\"FC1 ---> Bias    : \",params[1].size())\n",
    "\n",
    "print(\"FC2 ---> Weights : \",params[2].size())\n",
    "print(\"FC2 ---> Bias    : \",params[3].size())\n",
    "\n",
    "print(\"FC3 ---> Weights : \",params[4].size())\n",
    "print(\"FC3 ---> Bias    : \",params[5].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Accuracy of the network on the 10000 test images: 81 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    outputs = model(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    labels = labels\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print(total)\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
